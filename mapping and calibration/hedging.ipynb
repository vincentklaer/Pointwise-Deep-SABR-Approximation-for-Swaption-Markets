{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69343bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import functions.calibration as cal\n",
    "from functions.estimators import Hagan\n",
    "from functions.estimators import NeuralNetwork\n",
    "from functions.estimators import Antonov\n",
    "from py_vollib.black.greeks.analytical import delta\n",
    "from py_vollib.black.greeks.analytical import vega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b1c33",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f77745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import calibrated datasets and Monte Carlo generated hedges\n",
    "with open(\"mapping and calibration/files/calibrated_chunks_hagan.pkl\", \"rb\") as f:\n",
    "    chunks_hagan = pickle.load(f)\n",
    "with open(\"mapping and calibration/files/calibrated_chunks_antonov.pkl\", \"rb\") as f:\n",
    "    chunks_antonov = pickle.load(f)\n",
    "with open(\"mapping and calibration/files/calibrated_chunks_nn.pkl\", \"rb\") as f:\n",
    "    chunks_nn = pickle.load(f)\n",
    "with open(\"data generation/test pickles/recomputed_dataset_hedging.pkl\", \"rb\") as f:\n",
    "    chunks_mc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters from dictionary\n",
    "for approximator, chunks in {\"hagan\": chunks_hagan, \"antonov\": chunks_antonov, \"nn\": chunks_nn}.items():\n",
    "    for _, df in enumerate(chunks):\n",
    "        df[f\"alpha_{approximator}\"] = df[f\"params_{approximator}\"].apply(lambda x: x[\"alpha\"])\n",
    "        df[f\"rho_{approximator}\"] = df[f\"params_{approximator}\"].apply(lambda x: x[\"rho\"])\n",
    "        df[f\"nu_{approximator}\"] = df[f\"params_{approximator}\"].apply(lambda x: x[\"v\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd02d76",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chunks = []\n",
    "\n",
    "# Loop through each corresponding set of chunks\n",
    "for i, (df_hagan, df_antonov, df_nn) in enumerate(zip(chunks_hagan, chunks_antonov, chunks_nn)):\n",
    "\n",
    "    # Combine dataframes and remove duplicates\n",
    "    combined_df = pd.concat([df_hagan, df_antonov, df_nn], axis=1)\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]  # drop duplicate columns\n",
    "    combined_chunks.append(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Monte Carlo generated hedging datasets with calibrated datasets of the different approximator\n",
    "\n",
    "tol=1e-6\n",
    "\n",
    "merged_chunks = []\n",
    "\n",
    "for i, mc_chunk in enumerate(chunks_mc):\n",
    "    # Extract the reference parameter set\n",
    "    alpha_ref, nu_ref, rho_ref = mc_chunk[['alpha', 'nu', 'rho']].iloc[0].values\n",
    "\n",
    "    # Try to find a matching chunk in combined_chunks\n",
    "    match_idx = None\n",
    "    for j, comb_chunk in enumerate(combined_chunks):\n",
    "        a, n, r = comb_chunk[['alpha', 'nu', 'rho']].iloc[0].values\n",
    "        if (abs(a - alpha_ref) <= tol and \n",
    "            abs(n - nu_ref) <= tol and \n",
    "            abs(r - rho_ref) <= tol):\n",
    "            match_idx = j\n",
    "            break\n",
    "    \n",
    "    # Skip if no match found\n",
    "    if match_idx is None:\n",
    "        continue\n",
    "    \n",
    "    matched_chunk = combined_chunks[match_idx]\n",
    "\n",
    "    # Keep only rows with common strike prices\n",
    "    common_strikes = mc_chunk['strike_price'].isin(matched_chunk['strike_price'])\n",
    "    mc_chunk_filtered = mc_chunk[common_strikes].copy()\n",
    "\n",
    "    # Identify new columns\n",
    "    new_cols = [c for c in matched_chunk.columns if c not in mc_chunk_filtered.columns or c == 'strike_price']\n",
    "\n",
    "    # Merge only on common strike prices\n",
    "    merged = pd.merge(\n",
    "        mc_chunk_filtered,\n",
    "        matched_chunk[new_cols],\n",
    "        on='strike_price',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Add to result\n",
    "    merged_chunks.append(merged)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268f7e4",
   "metadata": {},
   "source": [
    "### Hagan Hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivatives with regards to f, alpha, rho, and nu of the implied volatilities using central differences approximation\n",
    "processed_hagan, failed_hagan = cal.IV_derivative(merged_chunks, Hagan, \"hagan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8184c",
   "metadata": {},
   "source": [
    "### Neural Network Hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivatives with regards to f, alpha, rho, and nu of the implied volatilities using central differences approximation\n",
    "processed_nn, failed_nn = cal.IV_derivative(merged_chunks, NeuralNetwork, \"nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c145b8",
   "metadata": {},
   "source": [
    "### ZC Map Hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d125be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivatives with regards to f, alpha, rho, and nu of the implied volatilities using central differences approximation\n",
    "processed_antonov, failed_antonov = cal.IV_derivative(merged_chunks, Antonov, \"antonov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0716b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes with hedges\n",
    "with open(\"hedged_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(merged_chunks, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
